{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbl9TP5NGkQa8Rj/3XI5pC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GydeokLee/dl23/blob/main/%EB%94%94%EC%A7%80%ED%84%B8%EB%A6%AC%ED%84%B0%EB%9F%AC%EC%8B%9C%20%EC%B0%B8%EA%B3%A0%EC%9E%90%EB%A3%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tGTTS ì„¤ì¹˜ ì½”ë“œ : !pip install Gtts\n",
        "                 !pip install iPython\n",
        "1.\tFrom gtts import gtts\n",
        "2.\tì˜¤ë””ì˜¤ ì„¤ì¹˜ ì½”ë“œ: from IPython.display import Audio ì´ ì¤„ê¹Œì§€ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í•´ì•¼í•¨\n",
        "1.\tdef tts(text):\n",
        "2.\tgtts_object = gTTs(text, lang = â€œenâ€ , slow = True)\n",
        "3.\tgtts_object.save(:myaudio.mp3â€)\n",
        "4.\treturn Audio(â€œmyaudio.mp3â€) ì—¬ê¸°ê¹Œì§€ ì˜¤ë””ì˜¤ íŒŒì¼ ë§Œë“¤ê¸°\n",
        "5.\tì´ê¹Œì§€ í–ˆìœ¼ë©´ ì˜¤ë””ì˜¤ ì¬ìƒ í•  ë•Œ tts(â€˜somethingâ€) ìƒˆ ì½”ë“œì— ë§Œë“¤ë©´ ë¨.\n",
        "6.\tmytext = input(â€œType text to generate audio: â€œ)\n",
        "7.\ttts(mytext) í•˜ë©´ ë‚´ê°€ ì…ë ¥í•˜ëŠ” ì–¸ì–´ë¥¼ ì¬ìƒí•  ìˆ˜ ìˆê²Œ ì…ë ¥ì°½ì´ ë‚˜ì˜¤ëŠ” ì½”ë“œê°€ ìƒì„±ì´ ëœë‹¤â€™. Etts, ktts, ftts(â€œsomethingâ€)ì„ ì½”ë“œë¡œ ë§Œë“¤ë©´ ê·¸ ì–¸ì–´ì— ë§ëŠ” ì–¸ì–´ íŒŒì¼ì´ ì¬ìƒëœë‹¤.\n",
        "8.\t#ì€ ì¸ê°„ë§Œì´ ì•Œì•„ë“¤ì„ ìˆ˜ ìˆëŠ” ë§ì´ë‹¤\n"
      ],
      "metadata": {
        "id": "-A64bV1TRi_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  variation: ë³€ìˆ˜, ì½”ë”©í•  ë•Œ í•„ìš”í•œ ë³€ìˆ˜ë¥¼ ì§€ì •í•˜ëŠ” ê²ƒ\n",
        "            Ex) x1 = 10\n",
        "               y1 â€œHello, class!â€ ì´ë ‡ê²Œ ì •ì˜ë¥¼ í•  ìˆ˜ ìˆë‹¤\n",
        "â€œâ€ ëŠ” ì½”ë”© ì‹œì— ì¸ê°„ì´ ì•Œì•„ë“¤ì„ ìˆ˜ ìˆëŠ” ì–¸ì–´ë¥¼ ì˜ë¯¸í•œë‹¤ ì•ˆí•˜ë©´ ì»´í“¨í„°ëŠ” ì»´í“¨í„° ì–¸ì–´ë¡œ ì•Œì•„ë“£ëŠ”ë‹¤.\n",
        "x1 = x + 10 ì´ë¼ëŠ” ì‹ì„ ì½”ë”©í•˜ë©´ 100ì´ë¼ëŠ” ê°’ì´ ë‚˜ì˜¨ë‹¤\n",
        "y1 = y + â€œWelcome to the class.â€ ë¼ê³  ë„£ìœ¼ë©´ hello, class! Welcome to the class. ë¼ê³  ë‚˜ì˜¨ë‹¤ ì´ë•Œ ë„ì–´ì“°ê¸° í•˜ê³  ì‹¶ìœ¼ë©´ â€˜â€ ë„£ìœ¼ë©´ ë„ì–´ì“°ê¸° ê°€ëŠ¥\n",
        "ê³„ì‚°ì‹ì€ x1 = 2;  y = 5\n",
        "         a1 = x +, -, * /ë¥¼ ì‚¬ìš©í•˜ë©´ ë˜ê³ , **ëŠ” ê±°ë“­ì œê³±ì„ ì˜ë¯¸í•˜ëŠ” ê²ƒì´ë‹¤.\n",
        "Print(something)ì„ ë§¨ ë°‘ì— ë„£ìœ¼ë©´ í•´ë‹¹ë˜ëŠ” ì‹ì— ëŒ€í•œ ê²°ê³¼ê°’ì´ ë‚˜ì˜¨ë‹¤\n",
        "def add(x, y): ëŠ” ë”í•˜ê¸°\n"
      ],
      "metadata": {
        "id": "rdaQzBOLRnbj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "return x + y\n",
        "def subtract(x, y): ëŠ” ë¹¼ê¸°\n",
        "return x- y\n",
        "def multiply(x, y) ëŠ” ê³±í•˜ê¸°\n",
        "return x * y\n",
        "def divide(x, y): ëŠ” ë‚˜ëˆ„ê¸° ë¼ê³  ì •ì˜ë¥¼ í•´ë‘ë©´\n",
        "return x / y\n",
        "add(2,3), subtract(2,3), multiply(2,3), divide(3,1) ë¼ê³  ì½”ë“œ ì§œë„£ìœ¼ë©´ ëœë‹¤\n",
        "\n",
        "Data typeì—ëŠ” ì—¬ëŸ¬ê°€ì§€ê°€ ìˆëŠ”ë°\n",
        "Numberì—ëŠ” int, floatì´ ìˆëŠ”ë° intëŠ” ì •ìˆ˜, floatëŠ” ì†Œìˆ˜ì´ë‹¤\n",
        "Stringì€ string ë¬¸ìë¥¼ ë§í•˜ëŠ” ê±°ì„\n",
        "ListëŠ” ì—¬ëŸ¬ê°€ì§€ ë¬¸ìë“¤ì´ ìˆëŠ” ìˆ˜ì—´ì„ ë§í•˜ëŠ” ê²ƒì´ë‹¤\n",
        "DictionaryëŠ” í‚¤(KEY)ì™€ ê°’(VALUE) ìŒì„ í¬í•¨í•˜ëŠ” ë°ì´í„°ì„ ì´ê±° ê°€ì§€ê³  í•™ìƒë“¤ ì ìˆ˜ë¥¼ ê´€ë¦¬í•˜ëŠ” ê±°ì„\n",
        "Tupleì€ ë³€ê²½í•  ìˆ˜ ì—†ëŠ” ìˆœì„œí˜• ë°ì´í„° êµ¬ì¡° ë¦¬ìŠ¤íŠ¸ë‘ ë¹„ìŠ·í•˜ì§€ë§Œ í•œ ë²ˆ ìƒì„±ëœ íŠœí”Œì€ ë³€ê²½í•  ìˆ˜ ì—†ìŒ\n",
        "Type()ëŠ” ê´„í˜¸ ì•ˆì— ìˆëŠ” ê²ƒì´ ë­” ì§€ ì•Œë ¤ì£¼ëŠ” ê±°ì„ intì¸ì§€ strì¸ì§€ ì•Œë ¤ì¤Œ\n",
        "ListëŠ” a = ì•ˆì— ìˆëŠ” êµ¬ì„±ìš”ì†Œë“¤ì„ ê°€ì§€ê³  ìˆëŠ” ê²ƒì„ ë§í•˜ëŠ” ê±°ì„\n",
        "random.randint(1, 10)ëœë¤ìœ¼ë¡œ ìˆ«ìë¥¼ ë‚˜ì˜¤ê²Œ í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜\n",
        "[]: listë¥¼ ë§í•˜ëŠ” ê±°ì„\n"
      ],
      "metadata": {
        "id": "fQQhCktLRw1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "API: application programming interface\n",
        "GPT: Generative Pre-trained Transformer\n",
        "í•œ í•¨ìˆ˜ì— ëŒ€í•´ì„œ ì •ì˜ë¥¼ í•´ ë‘ê³ , x3 = range(1, 10) ë¼ê³  í•˜ë©´ ì•ˆì— ìˆëŠ”ê±° ë‹¤ ë‚˜ì˜´\n",
        "                               List(x3)\n",
        "\n",
        "Mydict = {}ë¼ê³  ì •ì˜ í•´ë‘ \n",
        "Yournumber = input(something) ì´ë¼ê³  í•´ë‘ë©´ ìì‹ ì˜ ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ë©´ ë²ˆí˜¸ ë‚˜ì˜¤ê²Œ í•  ìˆ˜ ìˆìŒ\n",
        "Int, strìœ¼ë¡œ ë°”ê¾¸ë©´ ê·¸ê±¸ë¡œ ë°”ê¿€ ìˆ˜ ìˆìœ¼ë‹ˆê¹Œ\n",
        "Stringì€ integerë¡œ ë°”ê¿€ ìˆ˜ ì—†ë‹¤ integerëŠ” stringìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆë‹¤. â€œâ€ë¡œ í•˜ë©´ ë¨\n",
        "Yournumber = intâ€(yournumberâ€)ì´ë¼ê³  ë°”ê¾¸ë©´ intë¼ê³  ì¸ì‹ì„ í•˜ë‹ˆê¹Œ ê·¸ë ‡ê²Œ ì•Œë©´ ë¨\n",
        "Ex) day = input(â€œHow many days?: â€œ)\n",
        "Num_day = int(day)\n",
        "     Hours = 24\n",
        "     Total = num_day * hours\n",
        "     Print(â€œThe total hours passed since Mary met John are: â€œ, total, â€œhours.â€)\n"
      ],
      "metadata": {
        "id": "HqrrhE8HR7yp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strip()ì€ ê³µë°±ì´ë‚˜ tabs ê°™ì€ ê²ƒë“¤ì„ ì—†ì•  ì£¼ëŠ” ì—­í• ì´ë‹¤\n",
        "w1.split()ì€ ì•ˆì— ìˆëŠ” ê²ƒë“¤ space ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ ì£¼ëŠ” ê±°ì„\n",
        "joinì€ í•©ì³ì£¼ëŠ” ê²ƒì„ ë§í•˜ëŠ” ê±°ì„\n",
        "â€˜-â€˜,join(wordlist) ì´ëŸ° ì‹ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ëœë‹¤\n",
        "w1.lower() ì†Œë¬¸ì\n",
        "w1.upper() ëŒ€ë¬¸ì\n",
        "len(w1) : stringì˜ ê¸¸ì´\n",
        "w1[0] ì€ ì²«ë²ˆì§¸\n",
        "-1ì€ ë’¤ì—ì„œë¶€í„°\n",
        "0:ì€ 0ë¶€í„° ì˜¤ë¥¸ìª½ì— ìˆëŠ”ê±° ë‹¤\n",
        ":ì€ ì‹¹ ë‹¤\n",
        "1:3ì€ 1ë¶€í„° 3ê¹Œì§€ ì˜¤ë¥¸ìª½ì€ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤\n",
        "Numbers = [1,2,3,4,5]\n",
        "Squared = []\n",
        "For I in numbers:\n",
        "S = I ** 2\n",
        "Squared.append(s)\n",
        "Print(squared)\n",
        "Append()ëŠ” ë¶€ë¡ì²˜ëŸ¼ ë“¤ì–´ê°€ëŠ” ê²ƒì„ ë§í•œë‹¤\n"
      ],
      "metadata": {
        "id": "aeKZMSVGR_Wk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "W = â€œMary is friendlyâ€\n",
        "A = w.split()\n",
        "W1 = a[0]\n",
        "W2 = a[1]\n",
        "W3 = a[2]\n",
        "Nw1 = nw1 + â€œ â€œ + w1 + â€œ â€ +w3\n",
        "\n",
        "S1 = s[â€˜-1]\n",
        "S2 = s1 + â€œ?â€; s2\n"
      ],
      "metadata": {
        "id": "wQldiMJKSEVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import nltk\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "url = ì—¬ê¸°ì— ë§í¬ë§Œ ë„£ìœ¼ë¨„ ëœë‹¤\n",
        "os.system(\"curl \" + url + \" > RE.ch05.txt\")\n",
        "#@markdown ğŸŒ€ read a text file in the server: as _text_\n",
        "file = open(\"RE.ch05.txt\")\n",
        "text = file.read().replace(\"\\n\", \" \")\n",
        "file.close()\n",
        "\n",
        "markdownë¶€í„° fileê¹Œì§€ ì§€ìš°ë©´ ì œëª©ì—†ì´ ê·¸ë¦¼ë§Œ ë‚˜ì˜¨\n",
        "\n",
        "wc = WordCloud().generate(text)\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "g_gSe8f_SRNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Getting text from the user\n",
        "user_text = input(\"Please paste your text here: \")\n",
        "\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    background_color='white',\n",
        "    width=800,\n",
        "    height=800,\n",
        "    max_words=100\n",
        ").generate(user_text)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U7Qo2J7gSgTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë‹¨ì–´ ë³„ë¡œ splití•˜ëŠ” ë°©ë²•: example_stringì œì‹œ\n",
        "                set1 = example_string.split()\n",
        "                print(set1) í•˜ê¸°\n",
        "ë¬¸ì¥ ë³„ë¡œ splití•˜ëŠ” ë°©ë²•: set2 = example_string.split(\".\")\n",
        "set2\n",
        "\n",
        "Strip() í•˜ëŠ” ë°©ë²•\n",
        "\n",
        "set3 = [s.strip() for s in set2]\n",
        "set3"
      ],
      "metadata": {
        "id": "kKGq9DwjSgQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk library ì‚¬ìš©í•˜ëŠ” ë²•\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "slist = sent_tokenize(example_string)\n",
        "slist[0]\n",
        "\n",
        "í…ìŠ¤íŠ¸ ë„£ìœ¼ë©´ ëª‡ ê°œì¸ì§€ ì„¸\n",
        "mytext=input(\"Paste text: \")\n",
        "slist1 = sent_tokenize(mytext)\n",
        "slist1[50]\n",
        "\n",
        "mywords = word_tokenize(mytext)\n",
        "print(len(mywords))\n",
        "mywords[100]\n"
      ],
      "metadata": {
        "id": "7u9a6_r-Ubah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "sample = \"Sir, I protest. I am not a merry man!\"\n",
        "s1 = word_tokenize(sample); s1\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "w1 = \"Mary\"\n",
        "w2 = \"susan\"\n",
        "w1.casefold(): ëª¨ë“  ë¬¸ìë¥¼ ì†Œë¬¸ìí™”í•´ì„œ ë¹„êµí•˜ê¸° ì‰½ê²Œí•¨\n",
        "\n",
        "ì´ëŸ¬ë©´ sir, protest, merry, manë§Œ ë‚¨ëŠ”\n",
        "filtered_list = []\n",
        "\n",
        "for word in s1:\n",
        "   if (word.casefold() not in stop_words) & (len(word)>1):\n",
        "        filtered_list.append(word)\n",
        "\n",
        "filtered_list\n",
        "\n",
        "ì´ëŸ¬ë©´ englishì˜ ì¡°ê±´ì— ë¶€í•©í•˜ëŠ”ê±° ë¹¼ê³  ë‚˜\n",
        "w1 = word_tokenize(text1)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "w2 = [w for w in w1 if w.lower() not in stop_words and len(w) > 2]\n",
        "w2\n",
        "\n",
        "ì¶”ê°€í•˜ê¸°\n",
        "additional_stopwords = {'one', 'day', 'upon'}\n",
        "stop_words = set(stopwords.words('english') + list(additional_stopwords))\n",
        "w1 = word_tokenize(text1)\n",
        "stop_words = set(stopwords.words('english') + list(additional_stopwords))\n",
        "\n",
        "w2 = [w for w in w1 if w.lower() not in stop_words and len(w) > 2]\n",
        "w2\n",
        "\n",
        "Tagging\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = input(\"Type your sentence: \")\n",
        "mywords = word_tokenize(sentence)\n",
        "nltk.pos_tag(mywords)\n",
        "\n",
        "nltk.download('tagsets')\n",
        "\n",
        "\n",
        "from nltk.book import *\n",
        "\n",
        "\n",
        "text8.concordance(\"man\")\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Define some plain text\n",
        "text = input(\"Paste your text: \")\n",
        "\n",
        "# Tokenize the text into a list of words\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Create an nltk.Text object from the list of tokens\n",
        "text_object = nltk.Text(tokens)\n",
        "\n",
        "# Use the concordance method to find occurrences of the word \"fox\"\n",
        "text_object.concordance(\"be\")"
      ],
      "metadata": {
        "id": "AfKVNwkeVwFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nltk ê°€ì ¸ì˜¤\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "ë§ˆì¹¨í‘œë¹¼ê³  ê¸€ì ìˆ˜ ì²´í¬í•˜\n",
        "words = word_tokenize(text)\n",
        "wordlist = []\n",
        "for w in words:\n",
        "  if len(w) > 2:\n",
        "    wordlist.append(w)\n",
        "    print('Total words including punctuation: ', len(words))\n",
        "print('Total words: ', len(wordlist))\n",
        "print(wordlist)\n",
        "\n",
        "ë‹¤ ì†Œë¬¸ìí™”\n",
        "owerword = []\n",
        "\n",
        "for w in wordlist:\n",
        "  w1 = w.lower()\n",
        "  lowerword.append(w1)\n",
        "print(lowerword)\n",
        "\n",
        "words = lowerword\n",
        "print(\"Word size before stopwords: \", len(words))\n",
        "\n",
        "\n",
        "stop words to remove\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(\"Word size before stopwords: \", len(words))\n",
        "print(words)\n",
        "\n",
        "print(\"=\"*50)\n",
        "words2 = [w for w in words if w not in stopwords.words('english')]\n",
        "print(words2)\n",
        "print(\"Word size after removing stopwords: \", len(words2))"
      ],
      "metadata": {
        "id": "kmlCKaV9YXTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fd = nltk.FreqDist(words2)\n",
        "fd\n",
        "print('Word type: %d'%len(fd),'ê°œ')\n",
        "\n",
        "fwords = nltk.FreqDist(words2).most_common()\n",
        "print(fwords[:5])\n",
        "ì´ê±°ì— ëŒ€í•œ ê²°ê³¼ê°’\n",
        "Word type: 213 ê°œ\n",
        "[('light', 22), ('ocean', 6), ('shrimp', 5), ('life', 4), ('organisms', 4)]"
      ],
      "metadata": {
        "id": "l0ziyrVkWpZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One time occurrences:\n",
        "freq_num1 = [item[1] for item in fwords]\n",
        "num1 = freq_num1.count(1);\n",
        "print('Number of words that occur only once: %d'%num1)\n",
        "print('The rest: %d'%(213-num1))\n",
        "\n",
        "# nth word that occurs more than 2\n",
        "i=0\n",
        "for n in freq_num1:\n",
        "  if n >2:\n",
        "#    print(n)\n",
        "    i += 1\n",
        "print('ëª‡ ë²ˆì§¸ ë‹¨ì–´ê¹Œì§€ 2íšŒ ì´ˆê³¼ ë¹ˆë„? %d'%i, 'ë²ˆì§¸')\n",
        "\n",
        "\n",
        "ì´ê±°ì— ëŒ€í•œ ê²°ê³¼\n",
        "Number of words that occur only once: 164\n",
        "The rest: 49\n",
        "ëª‡ ë²ˆì§¸ ë‹¨ì–´ê¹Œì§€ 2íšŒ ì´ˆê³¼ ë¹ˆë„? 16 ë²ˆì§¸"
      ],
      "metadata": {
        "id": "G5FeP2aUaqin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¹ˆë²ˆë„ ì²´í¬í•˜ê¸°\n",
        "\n",
        "import csv\n",
        "fwords = nltk.FreqDist(words2).most_common()\n",
        "\n",
        "List_columns = ['Word', 'Freqeuncy']\n",
        "List_rows = fwords\n",
        "with open('Ch02_wordlist.csv', 'w') as csvfile:\n",
        "    write = csv.writer(csvfile)\n",
        "    write.writerow(List_columns)\n",
        "    write.writerows(List_rows)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"Ch02_wordlist.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "IzUtGvoTa3pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "gradio: machine learning model\n",
        "\n",
        "!pip install gradio\n",
        "\n",
        "ì˜ˆ\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def greet(name):\n",
        "    return \"Hello \" + name + \"!\"\n",
        "\n",
        "demo = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")\n",
        "\n",
        "demo.launch()\n",
        "\n",
        "\n",
        "ë‚ ì§œ ì„¸ê¸° ì–´í”Œ\n",
        "\n",
        "import gradio as gr\n",
        "from datetime import datetime\n",
        "\n",
        "def remaining_days(future_date: str):\n",
        "    try:\n",
        "        future_date = datetime.strptime(future_date, '%Y-%m-%d')\n",
        "    except ValueError:\n",
        "        return \"Invalid date format. Please use 'YYYY-MM-DD'\"\n",
        "\n",
        "    current_date = datetime.now()\n",
        "\n",
        "    remaining = future_date - current_date\n",
        "    return remaining.days\n",
        "\n",
        "iface = gr.Interface(fn=remaining_days,\n",
        "                     inputs=gr.inputs.Textbox(label=\"Input a future date (YYYY-MM-DD)\"),\n",
        "                     outputs=\"number\")\n",
        "iface.launch(share=True)\n",
        "\n",
        "\n",
        "\n",
        "ì˜¤ë””ì˜¤ë¡œ ë§Œë“¤ê¸°\n",
        "\n",
        "import gradio as gr\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "import os\n",
        "\n",
        "def text_to_speech(text):\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "    mp3_filename = \"voice.mp3\"\n",
        "    wav_filename = \"voice.wav\"\n",
        "    tts.save(mp3_filename)\n",
        "\n",
        "    # Convert from MP3 to WAV\n",
        "    audio = AudioSegment.from_mp3(mp3_filename)\n",
        "    audio.export(wav_filename, format=\"wav\")\n",
        "\n",
        "    return wav_filename\n",
        "\n",
        "iface = gr.Interface(fn=text_to_speech,\n",
        "                     inputs=gr.inputs.Textbox(),\n",
        "                     outputs=gr.outputs.Audio(type=\"filepath\"))\n",
        "iface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "hMwk0OIPbOmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "grade check\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# The 10 most popular baby names\n",
        "popular_baby_names = [\"Liam\", \"Emma\", \"Noah\", \"Olivia\", \"Ava\", \"Isabella\", \"Sophia\", \"Mia\", \"Charlotte\", \"Amelia\"]\n",
        "\n",
        "# Create lists for the data\n",
        "names = popular_baby_names\n",
        "english_grades = [random.randint(0, 100) for _ in range(10)]\n",
        "math_grades = [random.randint(0, 100) for _ in range(10)]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'Name': names,\n",
        "    'English': english_grades,\n",
        "    'Math': math_grades,\n",
        "})\n",
        "\n",
        "print(df)\n",
        "\n",
        "ë°‘ì—êº¼ëŠ” ì´ë¦„ ë„£ìœ¼ë©´ outputì´ ë‚˜ì˜¨\n",
        "\n",
        "# !pip install gradio\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "def check_grades(name):\n",
        "    name = name.strip().lower()  # remove extra white spaces and convert to lower case\n",
        "    df['Name'] = df['Name'].str.strip().str.lower()  # do the same for names in DataFrame\n",
        "    if name in df['Name'].values:\n",
        "        english_grade = df.loc[df['Name'] == name, 'English'].values[0]\n",
        "        math_grade = df.loc[df['Name'] == name, 'Math'].values[0]\n",
        "        return f\"English Grade: {english_grade}, Math Grade: {math_grade}\"\n",
        "    else:\n",
        "        return \"Name not found in the DataFrame.\"\n",
        "\n",
        "\n",
        "iface = gr.Interface(fn=check_grades,\n",
        "                     inputs=gr.inputs.Textbox(lines=1, placeholder='Enter a name...'),\n",
        "                     outputs=\"text\")\n",
        "iface.launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "vx2c_Pg4cxgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìœ íŠœë¸Œ ì˜ìƒ í‹€ê¸°\n",
        "!pip install gtts\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio, YouTubeVideo\n",
        "\n",
        "YouTubeVideo(\"_lBbs0kpYRs\", width=800, height=600)\n",
        "\n",
        "ì˜¤ë””ì˜¤ë¡œ ì½ì–´ì£¼ê¸°\n",
        "txt = \"He was heartbroken, realizing the mistake he had made.\"\n",
        "tts(txt)\n",
        "Audio('sample.mp3')\n",
        "\n",
        "\n",
        "\n",
        "!pip install openai\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive'\n",
        "\n",
        "import openai\n",
        "openai.api_key = input('openai API key here')\n",
        "\n",
        "\n",
        "í•œêµ­ì–´ë¡œ ì½ê¸°\n",
        "ktts(\"ì˜›ë‚ ì— ë³´ë¬¼ì„ ì„¸ìƒì—ì„œ ë¬´ì—‡ë³´ë‹¤ ì¢‹ì•„í•˜ëŠ” ë§ˆì´ë‹¤ìŠ¤ë¼ëŠ” ì™•ì´ ì‚´ê³  ìˆì—ˆìŠµë‹ˆë‹¤.\")\n",
        "Audio(\"Ksample.mp3\")"
      ],
      "metadata": {
        "id": "47m1iOLsdGMe"
      }
    }
  ]
}